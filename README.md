# SparseFusion

[**SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction**](https://sparsefusion.github.io/)<br/>
[Zhizhuo Zhou](https://www.zhiz.dev/),
[Shubham Tulsiani](https://shubhtuls.github.io/)<br/>
_[GitHub](https://github.com/zhizdev/sparsefusion) | [arXiv](https://sparsefusion.github.io/) | [Project page](https://sparsefusion.github.io/)_

![txt2img-stable2](media/teaser.jpg)
SparseFusion reconstructs a consistent and realistic 3D neural scene representation from as few as 2 input images with known relative pose. SparseFusion is able to generate detailed and plausible structures in uncertain or unobserved regions (such as front of the hydrant, teddybear's face, back of the laptop, or left side of the toybus).

## Code
We plan on releasing code and pretrained models in December 2022. Stay tuned for exciting updates!


## BibTeX

```
@misc{,
      title={}, 
      author={},
      year={},
      eprint={},
      archivePrefix={},
      primaryClass={}
}
```

## Acknowledgements 

We thank Naveen Venkat, Mayank Agarwal, Jeff Tan, Paritosh Mittal, Yen-Chi Cheng, and Nikolaos Gkanatsios for helpful discussions and feedback. We also thank David Novotny and Jonáš Kulhánek for sharing pretrained models for NerFormer and ViewFormer, respectively. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. (DGE1745016, DGE2140739).
